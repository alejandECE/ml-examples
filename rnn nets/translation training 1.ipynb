{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Created by Luis Alejandro (alejand@umich.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import unicode_to_ascii\n",
    "from dataset import DatasetBuilder\n",
    "from basic_translation import preprocess\n",
    "from basic_translation import Translator\n",
    "import os\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU?\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataset for training\n",
    "files = ['../../datasets/nlp/english-spanish.txt']\n",
    "builder = DatasetBuilder(files, preprocessors=(preprocess,preprocess), batch_size=64, max_obs=40000, test_obs=30)\n",
    "train_dataset, test_dataset = builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding matrix (glove vectors) -> these will only be used for English (source/input)\n",
    "embedding_size = 100\n",
    "word_to_vector = {}\n",
    "with open('../../datasets/glove.6B/glove.6B.%sd.txt' % embedding_size, encoding='utf8') as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = unicode_to_ascii(values[0])\n",
    "        vector = np.asarray(values[1:],dtype=np.float32)\n",
    "        word_to_vector[word] = vector\n",
    "        \n",
    "embedding_vectors = np.zeros((len(builder.source_tokenizer.word_to_index), embedding_size))\n",
    "for word, index in builder.source_tokenizer.word_to_index.items():\n",
    "    if word.decode() in word_to_vector:\n",
    "        embedding_vectors[index,:] = word_to_vector[word.decode()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model\n",
    "translator = Translator(builder.source_tokenizer,\n",
    "                        builder.target_tokenizer,\n",
    "                        source_embedding_matrix=embedding_vectors,\n",
    "                        target_embedding_size=embedding_size,\n",
    "                        max_output_length=builder.target_tokenizer.max_seq,\n",
    "                        restore=False,\n",
    "                        masking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 out of 15 complete (55.64 secs) -- Train Loss: 1902.9781 -- Train Bleu: 0.01\n",
      "Epoch 2 out of 15 complete (21.55 secs) -- Train Loss: 1326.9764 -- Train Bleu: 0.06\n",
      "Epoch 3 out of 15 complete (21.45 secs) -- Train Loss: 1104.8544 -- Train Bleu: 0.10\n",
      "Epoch 4 out of 15 complete (21.34 secs) -- Train Loss: 940.2594 -- Train Bleu: 0.14\n",
      "Epoch 5 out of 15 complete (21.48 secs) -- Train Loss: 807.7692 -- Train Bleu: 0.19\n",
      "Epoch 6 out of 15 complete (21.29 secs) -- Train Loss: 695.4304 -- Train Bleu: 0.24\n",
      "Epoch 7 out of 15 complete (21.56 secs) -- Train Loss: 599.1246 -- Train Bleu: 0.29\n",
      "Epoch 8 out of 15 complete (21.68 secs) -- Train Loss: 517.2880 -- Train Bleu: 0.35\n",
      "Epoch 9 out of 15 complete (21.57 secs) -- Train Loss: 448.2865 -- Train Bleu: 0.40\n",
      "Epoch 10 out of 15 complete (22.15 secs) -- Train Loss: 390.6614 -- Train Bleu: 0.46\n",
      "Creating intermediate checkpoint!\n",
      "\n",
      "Epoch 11 out of 15 complete (22.68 secs) -- Train Loss: 341.9345 -- Train Bleu: 0.51\n",
      "Epoch 12 out of 15 complete (23.24 secs) -- Train Loss: 301.9749 -- Train Bleu: 0.56\n",
      "Epoch 13 out of 15 complete (22.80 secs) -- Train Loss: 268.5747 -- Train Bleu: 0.60\n",
      "Epoch 14 out of 15 complete (22.45 secs) -- Train Loss: 240.7292 -- Train Bleu: 0.64\n",
      "Epoch 15 out of 15 complete (22.87 secs) -- Train Loss: 216.6384 -- Train Bleu: 0.66\n",
      "Creating final checkpoint!\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "translator.train(15, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: nadie sabe la razon .\n",
      "Translation: nadie sabe el chiste .\n",
      "\n",
      "Expected: ¿ tom esta aprendiendo frances ?\n",
      "Translation: ¿ esta tom frances a frances ?\n",
      "\n",
      "Expected: pense que tom estaba muerto .\n",
      "Translation: pense que tom estaba muerto .\n",
      "\n",
      "Expected: estoy en el toilette .\n",
      "Translation: estoy en el bano .\n",
      "\n",
      "Expected: ¿ quienes son tus padres ?\n",
      "Translation: ¿ quienes son tus amigos ?\n",
      "\n",
      "Expected: se quebro .\n",
      "Translation: se acabo todo .\n",
      "\n",
      "Expected: creo que me gustas .\n",
      "Translation: me gustas , no te .\n",
      "\n",
      "Expected: simplemente tiralo .\n",
      "Translation: simplemente lo intenta .\n",
      "\n",
      "Expected: esta novela es aburrida .\n",
      "Translation: este libro es muy pesado .\n",
      "\n",
      "Expected: la proxima manejo yo .\n",
      "Translation: de nuevo , viene , por casa .\n",
      "\n",
      "Expected: cordialmente gracias .\n",
      "Translation: muchisimas gracias .\n",
      "\n",
      "Expected: camino junto a ella .\n",
      "Translation: estoy pintando detras de mary .\n",
      "\n",
      "Expected: ¿ que tarda tanto ?\n",
      "Translation: ¿ que tan tan tan largo ?\n",
      "\n",
      "Expected: tenia un mal dia .\n",
      "Translation: tuve mucho menos .\n",
      "\n",
      "Expected: estoy en deuda contigo .\n",
      "Translation: estoy en deuda con el .\n",
      "\n",
      "Expected: ¿ estas cansado ?\n",
      "Translation: ¿ estas cansada ?\n",
      "\n",
      "Expected: abandono a su mujer .\n",
      "Translation: el se caso con su esposa .\n",
      "\n",
      "Expected: puede que vuelva .\n",
      "Translation: puede que vuelvas .\n",
      "\n",
      "Expected: a todos les agradas .\n",
      "Translation: a todos les agradas .\n",
      "\n",
      "Expected: ¿ quien pago ?\n",
      "Translation: ¿ quien lo ha pegado ?\n",
      "\n",
      "Expected: puedes usar este boligrafo .\n",
      "Translation: puedes usar este lapiz .\n",
      "\n",
      "Expected: tomas nunca supo eso .\n",
      "Translation: tom nunca supo aquello .\n",
      "\n",
      "Expected: estoy muy enojado .\n",
      "Translation: estoy muy enojada .\n",
      "\n",
      "Expected: ¿ como fue la fiesta de tom ?\n",
      "Translation: ¿ como estuvo el nuevo de tom ?\n",
      "\n",
      "Expected: toma , usa mi llave .\n",
      "Translation: aqui , mis profesores .\n",
      "\n",
      "Expected: tom vino muy tarde .\n",
      "Translation: tom vino demasiado tarde .\n",
      "\n",
      "Expected: ¿ quien es este muchacho ?\n",
      "Translation: ¿ quien es este chico ?\n",
      "\n",
      "Expected: compre un pasaje .\n",
      "Translation: compre un boleto .\n",
      "\n",
      "Expected: puede que no lo encuentres .\n",
      "Translation: podrias podrias intentarlo .\n",
      "\n",
      "Expected: ¿ sabeis hacer una ensalada ?\n",
      "Translation: ¿ puedes hacer una ensalada ?\n",
      "\n",
      "Bleu: 0.29550611338102806\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "translator.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 out of 15 complete (22.71 secs) -- Train Loss: 196.0225 -- Train Bleu: 0.69 -- Test Loss: 296.2232 -- Test Bleu: 0.35\n",
      "Epoch 2 out of 15 complete (22.57 secs) -- Train Loss: 178.6952 -- Train Bleu: 0.71 -- Test Loss: 312.1083 -- Test Bleu: 0.41\n",
      "Epoch 3 out of 15 complete (22.89 secs) -- Train Loss: 163.1738 -- Train Bleu: 0.73 -- Test Loss: 308.4829 -- Test Bleu: 0.36\n",
      "Epoch 4 out of 15 complete (22.38 secs) -- Train Loss: 149.8224 -- Train Bleu: 0.75 -- Test Loss: 320.1981 -- Test Bleu: 0.35\n",
      "Epoch 5 out of 15 complete (22.57 secs) -- Train Loss: 138.1030 -- Train Bleu: 0.77 -- Test Loss: 323.0579 -- Test Bleu: 0.35\n",
      "Epoch 6 out of 15 complete (23.12 secs) -- Train Loss: 127.7729 -- Train Bleu: 0.79 -- Test Loss: 323.1880 -- Test Bleu: 0.31\n",
      "Epoch 7 out of 15 complete (23.37 secs) -- Train Loss: 118.7120 -- Train Bleu: 0.80 -- Test Loss: 317.7484 -- Test Bleu: 0.42\n",
      "Epoch 8 out of 15 complete (22.57 secs) -- Train Loss: 111.2219 -- Train Bleu: 0.81 -- Test Loss: 325.5356 -- Test Bleu: 0.31\n",
      "Epoch 9 out of 15 complete (22.41 secs) -- Train Loss: 104.8194 -- Train Bleu: 0.82 -- Test Loss: 332.6988 -- Test Bleu: 0.34\n",
      "Epoch 10 out of 15 complete (22.66 secs) -- Train Loss: 98.2939 -- Train Bleu: 0.83 -- Test Loss: 331.6721 -- Test Bleu: 0.39\n",
      "Creating intermediate checkpoint!\n",
      "\n",
      "Epoch 11 out of 15 complete (22.34 secs) -- Train Loss: 93.7169 -- Train Bleu: 0.83 -- Test Loss: 344.2522 -- Test Bleu: 0.36\n",
      "Epoch 12 out of 15 complete (22.47 secs) -- Train Loss: 88.6138 -- Train Bleu: 0.84 -- Test Loss: 331.3595 -- Test Bleu: 0.41\n",
      "Epoch 13 out of 15 complete (21.71 secs) -- Train Loss: 84.8775 -- Train Bleu: 0.85 -- Test Loss: 338.9968 -- Test Bleu: 0.36\n",
      "Epoch 14 out of 15 complete (21.57 secs) -- Train Loss: 81.3007 -- Train Bleu: 0.85 -- Test Loss: 344.9512 -- Test Bleu: 0.37\n",
      "Epoch 15 out of 15 complete (21.55 secs) -- Train Loss: 78.2371 -- Train Bleu: 0.86 -- Test Loss: 369.2562 -- Test Bleu: 0.37\n",
      "Creating final checkpoint!\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "translator.train(15, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: nadie sabe la razon .\n",
      "Translation: nadie sabe la cosa .\n",
      "\n",
      "Expected: ¿ tom esta aprendiendo frances ?\n",
      "Translation: ¿ esta tom en frances ?\n",
      "\n",
      "Expected: pense que tom estaba muerto .\n",
      "Translation: pense que tom estaba muerto .\n",
      "\n",
      "Expected: estoy en el toilette .\n",
      "Translation: estoy en el bano .\n",
      "\n",
      "Expected: ¿ quienes son tus padres ?\n",
      "Translation: ¿ quienes son tus amigos ?\n",
      "\n",
      "Expected: se quebro .\n",
      "Translation: ha llegado todo .\n",
      "\n",
      "Expected: creo que me gustas .\n",
      "Translation: creo que me gusta .\n",
      "\n",
      "Expected: simplemente tiralo .\n",
      "Translation: por fin te ha pasado de alli .\n",
      "\n",
      "Expected: esta novela es aburrida .\n",
      "Translation: este libro es aburrido .\n",
      "\n",
      "Expected: la proxima manejo yo .\n",
      "Translation: de casa , tom va a llegar .\n",
      "\n",
      "Expected: cordialmente gracias .\n",
      "Translation: muchisimas gracias .\n",
      "\n",
      "Expected: camino junto a ella .\n",
      "Translation: estoy leyendo una vez .\n",
      "\n",
      "Expected: ¿ que tarda tanto ?\n",
      "Translation: ¿ que hace tanto calor ?\n",
      "\n",
      "Expected: tenia un mal dia .\n",
      "Translation: tuve mucho tiempo .\n",
      "\n",
      "Expected: estoy en deuda contigo .\n",
      "Translation: estoy en deuda contigo .\n",
      "\n",
      "Expected: ¿ estas cansado ?\n",
      "Translation: ¿ estas cansada ?\n",
      "\n",
      "Expected: abandono a su mujer .\n",
      "Translation: dejo a su mujer .\n",
      "\n",
      "Expected: puede que vuelva .\n",
      "Translation: quiza volvais .\n",
      "\n",
      "Expected: a todos les agradas .\n",
      "Translation: a todos les agradas .\n",
      "\n",
      "Expected: ¿ quien pago ?\n",
      "Translation: ¿ quien pago ?\n",
      "\n",
      "Expected: puedes usar este boligrafo .\n",
      "Translation: puedes usar este lapiz .\n",
      "\n",
      "Expected: tomas nunca supo eso .\n",
      "Translation: tom nunca supo aquello .\n",
      "\n",
      "Expected: estoy muy enojado .\n",
      "Translation: estoy muy enojada .\n",
      "\n",
      "Expected: ¿ como fue la fiesta de tom ?\n",
      "Translation: ¿ que tal estuvo la fiesta de tom ?\n",
      "\n",
      "Expected: toma , usa mi llave .\n",
      "Translation: aqui , abri mi auto .\n",
      "\n",
      "Expected: tom vino muy tarde .\n",
      "Translation: tom vino demasiado tarde .\n",
      "\n",
      "Expected: ¿ quien es este muchacho ?\n",
      "Translation: ¿ quien es este chico ?\n",
      "\n",
      "Expected: compre un pasaje .\n",
      "Translation: compre un boleto .\n",
      "\n",
      "Expected: puede que no lo encuentres .\n",
      "Translation: puede que lo puedes lograr .\n",
      "\n",
      "Expected: ¿ sabeis hacer una ensalada ?\n",
      "Translation: ¿ puedes hacer una ensalada ?\n",
      "\n",
      "Bleu: 0.3994074012676201\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "translator.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'<start>' b'i' b'm' b'very' b'happy' b'to' b'see' b'you' b'.' b'<end>'], shape=(10,), dtype=string)\n",
      "tf.Tensor([[  1  18  50 947 296 256 105 106   3   4   0   0]], shape=(1, 12), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Creating some input\n",
    "source = b\"I'm very happy to see you.\"\n",
    "source = preprocess(tf.constant(source))\n",
    "source = tf.strings.split(tf.constant(source))\n",
    "print(source)\n",
    "source = builder.source_tokenizer.encode(source.numpy())\n",
    "source = tf.constant(np.pad(source,(0,builder.source_tokenizer.max_seq - len(source))),\n",
    "                     shape=[1,builder.source_tokenizer.max_seq])\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me alegro que estes feliz . <end>\n"
     ]
    }
   ],
   "source": [
    "# Outputing model translation\n",
    "prediction = translator.translate(source)\n",
    "print(' '.join(builder.target_tokenizer.index_to_word[word].decode() for word in prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
