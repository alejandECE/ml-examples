{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Created by Luis Alejandro (alejand@umich.edu)\n",
    "A single forward and backward pass for a softmax regressor using graphs. Compares results to analytical computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.graphs.core import Param\n",
    "from utils.graphs.core import DataHolder\n",
    "from utils.graphs.core import Graph\n",
    "from utils.graphs.core import Operation\n",
    "\n",
    "from utils.graphs.nodes import linear_node\n",
    "from utils.graphs.nodes import bias_node\n",
    "from utils.graphs.nodes import softmax_node\n",
    "from utils.graphs.nodes import mce_node\n",
    "from utils.graphs.nodes import softmax_mce_node\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_iris()\n",
    "predictors = dataset['data']\n",
    "responses = dataset['target'].reshape(-1,1)\n",
    "m,d = predictors.shape\n",
    "n = len(np.unique(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnd.seed(1)\n",
    "X_node = DataHolder()\n",
    "y_node = DataHolder()\n",
    "w_node = Param((d,n))\n",
    "b_node = Param((1,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_node = linear_node(X_node,w_node)\n",
    "z_node = bias_node(r_node,b_node)\n",
    "# J_node = softmax_mce_node(z_node,y_node)\n",
    "h_node = softmax_node(z_node)\n",
    "J_node = mce_node(h_node,y_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.graphs.core.Graph at 0x271b6e28688>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph()\n",
    "g.build(J_node).initialize().feed({X_node:predictors, y_node:responses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.graphs.core.Graph at 0x271b6e28688>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "g.forward().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(elements):\n",
    "    shift = elements.max(axis=1).reshape(-1,1)\n",
    "    exp = np.exp(elements - shift)\n",
    "    return exp / exp.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "def compute_cost(w,X,y):\n",
    "    m,d = X.shape\n",
    "    W = w.reshape((d,-1))\n",
    "    prob = softmax(X.dot(W))\n",
    "    loglikelihood = np.log(prob[range(m),y.flatten()]).sum() \n",
    "    return -loglikelihood\n",
    "\n",
    "def compute_grad(w,X,y):\n",
    "    # Reshape the weights into d,n\n",
    "    m,d = X.shape\n",
    "    W = w.reshape((d,-1))\n",
    "    _,n = W.shape\n",
    "    # Evaluates the indicator function for y (one-hot-encoding)\n",
    "    indicator = np.zeros((m,n))\n",
    "    indicator[range(len(y)),y.flatten()] = 1\n",
    "    prob = softmax(X.dot(W))\n",
    "    # Builds gradient\n",
    "    diff = indicator - prob\n",
    "    grad = np.zeros((d,n))\n",
    "    for c in range(n):\n",
    "        grad[:,c] = -(X * diff[:,c].reshape((m,1))).sum(axis=0)\n",
    "    grad = grad.flatten()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((m,1)), predictors))\n",
    "w = np.vstack((b_node.value,w_node.value))\n",
    "y = y_node.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convetional: 255.47998200002803\n",
      "Graph: 255.47998200002803\n"
     ]
    }
   ],
   "source": [
    "print('Convetional:', compute_cost(w,X,y))\n",
    "print('Graph:', J_node.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conventional: [ -37.29247915  -25.85918224   63.15166139 -174.87365033 -146.50164898\n",
      "  321.37529931 -133.40061661  -70.5852348   203.98585141  -20.31994661\n",
      "  -93.68251643  114.00246304    5.22547629  -26.61624693   21.39077063]\n"
     ]
    }
   ],
   "source": [
    "print('Conventional:', compute_grad(w,X,y).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: [ -37.29247915  -25.85918224   63.15166139 -174.87365033 -146.50164898\n",
      "  321.37529931 -133.40061661  -70.5852348   203.98585141  -20.31994661\n",
      "  -93.68251643  114.00246304    5.22547629  -26.61624693   21.39077063]\n"
     ]
    }
   ],
   "source": [
    "print('Graph:', np.hstack((b_node.gradient.flatten(), w_node.gradient.flatten())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
