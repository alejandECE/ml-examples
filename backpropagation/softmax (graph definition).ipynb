{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Created by Luis A. Sanchez-Perez (alejand@umich.edu).\n",
    "<p><span style=\"color:green\"><b>Copyright &#169;</b> Do not distribute or use without authorization from author.</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single forward and backward pass for a softmax regressor using graphs. Compares results to analytical computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from graphs.core import Param\n",
    "from graphs.core import DataHolder\n",
    "from graphs.core import Graph\n",
    "from graphs.core import Operation\n",
    "from graphs.nodes import linear_node\n",
    "from graphs.nodes import bias_node\n",
    "from graphs.nodes import softmax_node\n",
    "from graphs.nodes import mce_node\n",
    "from graphs.nodes import softmax_mce_node\n",
    "from sklearn import datasets\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_iris()\n",
    "predictors = dataset['data']\n",
    "responses = dataset['target'].reshape(-1,1)\n",
    "m,d = predictors.shape\n",
    "n = len(np.unique(responses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating custom computational graph\n",
    "Here we use our custom toy implemenations and perform one forward and backward pass computing the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnd.seed(1)\n",
    "X_node = DataHolder()\n",
    "y_node = DataHolder()\n",
    "w_node = Param((d,n))\n",
    "b_node = Param((1,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_node = linear_node(X_node,w_node)\n",
    "z_node = bias_node(r_node,b_node)\n",
    "# J_node = softmax_mce_node(z_node,y_node)\n",
    "h_node = softmax_node(z_node)\n",
    "J_node = mce_node(h_node,y_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<graphs.core.Graph at 0x23384babf88>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph()\n",
    "g.build(J_node).initialize().feed({X_node:predictors, y_node:responses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<graphs.core.Graph at 0x23384babf88>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "g.forward().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical implementations\n",
    "Here we use the known formulas for softmax regression to compute cost and gradient. Notice that this is possible only because the graph for softmax regression is a really simple one. If we had a more complex graph deriving these formulas analytically will be impossible (or not viable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(elements):\n",
    "    shift = elements.max(axis=1).reshape(-1,1)\n",
    "    exp = np.exp(elements - shift)\n",
    "    return exp / exp.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "def compute_cost(w,X,y):\n",
    "    m,d = X.shape\n",
    "    W = w.reshape((d,-1))\n",
    "    prob = softmax(X.dot(W))\n",
    "    loglikelihood = np.log(prob[range(m),y.flatten()]).sum() \n",
    "    return -loglikelihood\n",
    "\n",
    "def compute_grad(w,X,y):\n",
    "    # Reshape the weights into d,n\n",
    "    m,d = X.shape\n",
    "    W = w.reshape((d,-1))\n",
    "    _,n = W.shape\n",
    "    # Evaluates the indicator function for y (one-hot-encoding)\n",
    "    indicator = np.zeros((m,n))\n",
    "    indicator[range(len(y)),y.flatten()] = 1\n",
    "    prob = softmax(X.dot(W))\n",
    "    # Builds gradient\n",
    "    diff = indicator - prob\n",
    "    grad = np.zeros((d,n))\n",
    "    for c in range(n):\n",
    "        grad[:,c] = -(X * diff[:,c].reshape((m,1))).sum(axis=0)\n",
    "    grad = grad.flatten()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((m,1)), predictors))\n",
    "w = np.vstack((b_node.value,w_node.value))\n",
    "y = y_node.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building tensorflow graph\n",
    "Here we use tensorflow to create the same computational graph and performs a forward and backward pass computing the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = tf.convert_to_tensor(predictors, dtype=tf.float64)\n",
    "y_tensor = tf.convert_to_tensor(responses.ravel(), dtype=tf.int64)\n",
    "w_tensor = tf.Variable(w_node.value, dtype=tf.float64)\n",
    "b_tensor = tf.Variable(b_node.value, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    r_tensor = tf.matmul(X_tensor, w_tensor)\n",
    "    z_tensor = tf.add(r_tensor, b_tensor)\n",
    "    J_tensor = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=z_tensor, labels=y_tensor))\n",
    "gradients = tape.gradient(J_tensor, [b_tensor, w_tensor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convetional: [584.72821298]\n",
      "Graph (Custom): [584.72821298]\n",
      "Tensorflow: [584.72821298]\n"
     ]
    }
   ],
   "source": [
    "print('Convetional:', compute_cost(w,X,y).ravel())\n",
    "print('Graph (Custom):', J_node.value.ravel())\n",
    "print('Tensorflow:', J_tensor.numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conventional: [  90.74210564  -49.93176266  -40.81034298  575.77571779 -296.43786207\n",
      " -279.33785572  259.1622831  -138.30173991 -120.86054318  463.91529332\n",
      " -212.79977319 -251.11552013  160.15901713  -66.24346762  -93.91554951]\n"
     ]
    }
   ],
   "source": [
    "print('Conventional:', compute_grad(w,X,y).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph (Custom): [  90.74210564  -49.93176266  -40.81034298  575.77571779 -296.43786207\n",
      " -279.33785572  259.1622831  -138.30173991 -120.86054318  463.91529332\n",
      " -212.79977319 -251.11552013  160.15901713  -66.24346762  -93.91554951]\n"
     ]
    }
   ],
   "source": [
    "print('Graph (Custom):', np.hstack((b_node.gradient.flatten(), w_node.gradient.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow: [  90.74210564  -49.93176266  -40.81034298  575.77571779 -296.43786207\n",
      " -279.33785572  259.1622831  -138.30173991 -120.86054318  463.91529332\n",
      " -212.79977319 -251.11552013  160.15901713  -66.24346762  -93.91554951]\n"
     ]
    }
   ],
   "source": [
    "print('Tensorflow:', np.hstack((gradients[0].numpy().flatten(), gradients[1].numpy().flatten())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
