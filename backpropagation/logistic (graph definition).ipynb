{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Created by Luis A. Sanchez-Perez (alejand@umich.edu).\n",
    "<p><span style=\"color:green\"><b>Copyright &#169;</b> Do not distribute or use without authorization from author.</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logistic regression forward and backward pass. Compares results to analytical computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from graphs.core import Param\n",
    "from graphs.core import DataHolder\n",
    "from graphs.core import Graph\n",
    "from graphs.core import Operation\n",
    "from graphs.nodes import linear_node\n",
    "from graphs.nodes import bias_node\n",
    "from graphs.nodes import sigmoid_node\n",
    "from graphs.nodes import bce_node\n",
    "from sklearn import datasets\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_iris()\n",
    "predictors = dataset['data']\n",
    "responses = dataset['target'].reshape(-1,1)\n",
    "responses[responses == 2] = 1\n",
    "m,d = predictors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating custom computational graph\n",
    "Here we use our custom toy implemenations and perform one forward and backward pass computing the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_node = DataHolder()\n",
    "y_node = DataHolder()\n",
    "w_node = Param(shape=(d,1))\n",
    "b_node = Param(shape=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_node = linear_node(X_node,w_node)\n",
    "z_node = bias_node(r_node,b_node)\n",
    "h_node = sigmoid_node(z_node)\n",
    "J_node = bce_node(h_node,y_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<graphs.core.Graph at 0x243a0e7f048>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph()\n",
    "g.build(J_node).initialize().feed({X_node:predictors, y_node:responses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<graphs.core.Graph at 0x243a0e7f048>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.forward().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical implementations\n",
    "Here we use the known formulas for logistic regression to compute cost and gradient. Notice that this is possible only because the graph for logistic regression is a really simple one. If we had a more complex graph deriving these formulas analytically will be impossible (or not viable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def compute_cost(w,X,y):\n",
    "    h = sigmoid(X.dot(w))\n",
    "    loglikelihood = sum([np.log(prob) if label else np.log(1-prob) for prob,label in zip(h,y)])\n",
    "    return -loglikelihood\n",
    "\n",
    "def compute_grad(w,X,y):\n",
    "    h = sigmoid(X.dot(w))\n",
    "    grad = (X.T).dot(h - y)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((predictors.shape[0],1)), predictors))\n",
    "w = np.vstack((b_node.value,w_node.value))\n",
    "y = y_node.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building tensorflow graph\n",
    "Here we use tensorflow to create the same computational graph and performs a forward and backward pass computing the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = tf.convert_to_tensor(predictors, dtype=tf.float64)\n",
    "y_tensor = tf.convert_to_tensor(responses, dtype=tf.int64)\n",
    "w_tensor = tf.Variable(w_node.value, dtype=tf.float64)\n",
    "b_tensor = tf.Variable(b_node.value, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "    y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "    return -tf.reduce_sum(\n",
    "        y_true * tf.math.log(y_pred) + (1. - y_true) * tf.math.log(1 - y_pred)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    r_tensor = tf.matmul(X_tensor, w_tensor)\n",
    "    z_tensor = tf.add(r_tensor, b_tensor)\n",
    "    h_tensor = tf.nn.sigmoid(z_tensor)\n",
    "    J_tensor = binary_cross_entropy(y_tensor, h_tensor)\n",
    "#     J_tensor = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#         logits=z_tensor, labels=tf.cast(y_tensor, z_tensor.dtype)\n",
    "#     ))\n",
    "gradients = tape.gradient(J_tensor, [b_tensor, w_tensor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convetional: [861.34107898]\n",
      "Graph (Custom): [861.34107898]\n",
      "Tensorflow: [861.34107898]\n"
     ]
    }
   ],
   "source": [
    "print('Convetional:', compute_cost(w,X,y).ravel())\n",
    "print('Graph (Custom):', J_node.value.ravel())\n",
    "print('Tensorflow:', J_tensor.numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conventional: [ -99.8646402  -625.51477989 -286.77156947 -490.32975782 -167.53788101]\n"
     ]
    }
   ],
   "source": [
    "print('Conventional:', compute_grad(w,X,y).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph (Custom): [ -99.8646402  -625.51477989 -286.77156947 -490.32975782 -167.53788101]\n"
     ]
    }
   ],
   "source": [
    "print('Graph (Custom):', np.hstack((b_node.gradient.ravel(), w_node.gradient.ravel())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow: [ -99.8646402  -625.51477989 -286.77156947 -490.32975782 -167.53788101]\n"
     ]
    }
   ],
   "source": [
    "print('Tensorflow:', np.hstack((gradients[0].numpy().ravel(), gradients[1].numpy().ravel())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
