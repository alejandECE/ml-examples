{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Created by Luis A. Sanchez-Perez (alejand@umich.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import norm \n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = datasets.load_iris()\n",
    "print(dataset.feature_names, end=\"\\n\")\n",
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data,dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.95526316 3.38157895 1.45789474 0.24736842]\n",
      " [5.875      2.72222222 4.2        1.31666667]\n",
      " [6.61315789 2.96842105 5.55       1.98421053]]\n",
      "[[0.10510388 0.14150277 0.02822715 0.01144045]\n",
      " [0.256875   0.09950618 0.21944445 0.03861111]\n",
      " [0.38430056 0.11268698 0.30460527 0.06554017]]\n"
     ]
    }
   ],
   "source": [
    "# Fitting Naive Bayes\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "print(classifier.theta_)\n",
    "print(classifier.sigma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38  0  0]\n",
      " [ 0 33  3]\n",
      " [ 0  2 36]]\n",
      "0.9553571428571429\n"
     ]
    }
   ],
   "source": [
    "# Predicting the training set results\n",
    "y_pred = classifier.predict(X_train)\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "print(cm)\n",
    "print(accuracy_score(y_train,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  0  0]\n",
      " [ 0 12  2]\n",
      " [ 0  1 11]]\n",
      "0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "# Predicting the test set results\n",
    "y_pred = classifier.predict(X_test,)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = np.array([2,1,3,1],ndmin=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.62801341e-34, 9.99999652e-01, 3.47954930e-07]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing sklearn model output to compare\n",
    "classifier.predict_proba(point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building mean and std matrices\n",
    "Notice here we are using list comprehension. Notice we work on the training set only. We build a mean and a std matrix where each row represent the values to define the likelihood pdf for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.95526316, 3.38157895, 1.45789474, 0.24736842],\n",
       "       [5.875     , 2.72222222, 4.2       , 1.31666667],\n",
       "       [6.61315789, 2.96842105, 5.55      , 1.98421053]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = np.array([X_train[y_train == i,:].mean(axis=0) for i in np.unique(y_train)])\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32419728, 0.37616854, 0.16800937, 0.10696001],\n",
       "       [0.50682837, 0.31544599, 0.46844898, 0.1964971 ],\n",
       "       [0.6199198 , 0.33568882, 0.55191056, 0.25600814]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = np.array([X_train[y_train == i,:].std(axis=0) for i in np.unique(y_train)])\n",
    "std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building priors vector\n",
    "We use frequency analysis to determine this. Notice we work on the training set only. We get one value per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3392857142857143, 0.32142857142857145, 0.3392857142857143]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior = [(y_train == i).sum()/len(y_train) for i in np.unique(y_train)]\n",
    "prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the likelihood\n",
    "We compute the likelihood of observing each feature value in the given input (this uses the proper pdf, that is the proper mean and std from the matrix). The result is a matrix the same size as the mean and std matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.11251351e-18, 2.09661381e-09, 1.20585322e-18, 6.60706261e-11],\n",
       "       [1.59481319e-13, 4.25918898e-07, 3.20126833e-02, 5.54109982e-01],\n",
       "       [6.07738720e-13, 4.05958540e-08, 1.67314670e-05, 9.62197289e-04]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood = norm.pdf(point,mean,std)\n",
    "likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the log-likelihood instead\n",
    "We compute the log likelihood. As you can see the values are not as small as the ones from the likelihood so we have less chance to underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-41.3399098 , -19.98294227, -41.25934429, -23.44029685],\n",
       "       [-29.4668496 , -14.66901689,  -3.4416231 ,  -0.59039209],\n",
       "       [-28.12903134, -17.01959989, -10.99821936,  -6.94629105]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglikelihood = np.log(likelihood)\n",
    "loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the posterior\n",
    "Here we first use the straight foward method and along the way we can observe how we get really small values and of course a bigger chance to underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.30510288e-56, 3.87292653e-22, 1.34760406e-28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator = likelihood.prod(axis=1) * prior\n",
    "numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-127.10340592,  -49.30286161,  -64.17405436])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lognumerator = loglikelihood.sum(axis=1) + np.log(prior)\n",
    "lognumerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.62799388e-34, 9.99999652e-01, 3.47954856e-07])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior = numerator/numerator.sum()\n",
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the log-posterior\n",
    "We compute the log posterior using two different ways to determine P(x). First we compute this in an 'unsafe' (less stable) way and then we use te logsumexp trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.872927878814349e-22"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.78005447e+01, -3.47954924e-07, -1.48711931e+01])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logposterior = lognumerator - np.log(numerator.sum()) # unsafe\n",
    "logposterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.62799388e-34, 9.99999652e-01, 3.47954856e-07])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior = np.exp(logposterior)\n",
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999933"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.78005447e+01, -3.47954916e-07, -1.48711931e+01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logposterior = lognumerator - logsumexp(lognumerator) # more stable\n",
    "logposterior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.62799388e-34, 9.99999652e-01, 3.47954856e-07])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior = np.exp(logposterior)\n",
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000004"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
