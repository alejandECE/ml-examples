{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Created by Luis A. Sanchez-Perez (alejand@umich.edu).\n",
    "<p><span style=\"color:green\"><b>Copyright &#169;</b> Do not distribute or use without authorization from author.</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads dataset\n",
    "dataset = sio.loadmat('../../datasets/classification/emails.mat')\n",
    "vocab = [element[0] for element in dataset['vocab'][0]]\n",
    "X = dataset['X']\n",
    "y = dataset['Y'].ravel()\n",
    "# Preprocessing\n",
    "X[X > 0] = 1\n",
    "# Splitting the dataset into the training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = BernoulliNB()\n",
    "mdl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1494    9]\n",
      " [  69  678]]\n",
      "0.9653333333333334\n"
     ]
    }
   ],
   "source": [
    "# Predicting the training set results\n",
    "y_pred = mdl.predict(X_train)\n",
    "# Making the Confusion Matrix\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "print(cm)\n",
    "print(accuracy_score(y_train,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[490   7]\n",
      " [ 36 217]]\n",
      "0.9426666666666667\n"
     ]
    }
   ],
   "source": [
    "# Predicting the test set results\n",
    "y_pred = mdl.predict(X_test)\n",
    "# Making the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_email(url, vocab):\n",
    "    corpus = dict(zip(vocab, np.zeros(len(vocab))))\n",
    "    with open(url,'r') as fid:\n",
    "        text = fid.read()\n",
    "#         print('Email: ', text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z\\'\\s]',' ',text)\n",
    "#         print('Preprocessed email: ', text)\n",
    "        for word in text.split():\n",
    "            if word in corpus:\n",
    "                corpus[word] = 1\n",
    "            else:\n",
    "                print('Word', '\"{}\"'.format(word), 'not in dictionary')\n",
    "        arr = np.array([corpus[element] for element in vocab])\n",
    "        print('List of words in arr:', [vocab[i] for i, value in enumerate(arr) if value])\n",
    "    return arr.reshape(1,len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \"of\" not in dictionary\n",
      "Word \"neurology\" not in dictionary\n",
      "Word \"neurological\" not in dictionary\n",
      "Word \"issn\" not in dictionary\n",
      "Word \"luis\" not in dictionary\n",
      "Word \"alejandro\" not in dictionary\n",
      "Word \"sanchezperez\" not in dictionary\n",
      "Word \"it\" not in dictionary\n",
      "Word \"is\" not in dictionary\n",
      "Word \"a\" not in dictionary\n",
      "Word \"to\" not in dictionary\n",
      "Word \"my\" not in dictionary\n",
      "Word \"to\" not in dictionary\n",
      "Word \"a\" not in dictionary\n",
      "Word \"scholar\" not in dictionary\n",
      "Word \"we\" not in dictionary\n",
      "Word \"on\" not in dictionary\n",
      "Word \"neuroscience\" not in dictionary\n",
      "Word \"neurological\" not in dictionary\n",
      "Word \"disorders\" not in dictionary\n",
      "Word \"is\" not in dictionary\n",
      "Word \"to\" not in dictionary\n",
      "Word \"of\" not in dictionary\n",
      "Word \"we\" not in dictionary\n",
      "Word \"to\" not in dictionary\n",
      "Word \"we\" not in dictionary\n",
      "Word \"be\" not in dictionary\n",
      "Word \"if\" not in dictionary\n",
      "Word \"us\" not in dictionary\n",
      "Word \"of\" not in dictionary\n",
      "Word \"clinical\" not in dictionary\n",
      "Word \"we\" not in dictionary\n",
      "Word \"to\" not in dictionary\n",
      "Word \"to\" not in dictionary\n",
      "Word \"as\" not in dictionary\n",
      "Word \"an\" not in dictionary\n",
      "Word \"on\" not in dictionary\n",
      "Word \"or\" not in dictionary\n",
      "Word \"september\" not in dictionary\n",
      "Word \"th\" not in dictionary\n",
      "Word \"at\" not in dictionary\n",
      "Word \"neurology\" not in dictionary\n",
      "Word \"peertechz\" not in dictionary\n",
      "Word \"us\" not in dictionary\n",
      "Word \"or\" not in dictionary\n",
      "Word \"as\" not in dictionary\n",
      "Word \"an\" not in dictionary\n",
      "Word \"thomas\" not in dictionary\n",
      "Word \"muller\" not in dictionary\n",
      "Word \"in\" not in dictionary\n",
      "Word \"amin\" not in dictionary\n",
      "Word \"g\" not in dictionary\n",
      "Word \"california\" not in dictionary\n",
      "Word \"usa\" not in dictionary\n",
      "Word \"id\" not in dictionary\n",
      "Word \"neurology\" not in dictionary\n",
      "Word \"peertechz\" not in dictionary\n",
      "Word \"us\" not in dictionary\n",
      "List of words in arr: ['the', 'and', 'you', 'for', 'this', 'your', 'with', 'are', 'will', 'can', 'email', 'any', 'which', 'like', 'please', 'most', 'may', 'help', 'report', 'best', 'before', 'back', 'future', 'great', 'looking', 'reply', 'case', 'put', 'interest', 'etc', 'welcome', 'area', 'currently', 'dear', 'review', 'research', 'forward', 'recent', 'request', 'regards', 'short', 'editor', 'communication', 'journal', 'submit', 'cases', 'science', 'chief', 'respond', 'opinion', 'regarding', 'articles', 'images', 'relevant', 'train', 'seeking', 'managing', 'glad', 'residents', 'challenges', 'rare', 'typical', 'expertise', 'pleasure', 'contributions', 'greetings', 'complicated', 'kindly', 'attachment', 'alt', 'awaiting', 'disorder', 'revert', 'manuscript', 'tackle']\n"
     ]
    }
   ],
   "source": [
    "arr = load_email('../../datasets/classification/email.txt', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.49092168e-07, 9.99999351e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing sklearn model output to compare\n",
    "mdl.predict_proba(arr.reshape(1,len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines number of times each feature was one per class\n",
    "is_ham = (y_train == 0)\n",
    "occurrences_ham = np.array(X_train[is_ham,:].sum(axis=0)).flatten()\n",
    "occurrences_spam = np.array(X_train[~is_ham,:].sum(axis=0)).flatten()\n",
    "occurrences_total = np.array(X_train.sum(axis=0)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(((occurrences_ham + occurrences_spam) == occurrences_total).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computes the probability of observing each feature being one per class\n",
    "prob_ham = occurrences_ham / is_ham.sum()\n",
    "prob_spam = occurrences_spam / (~is_ham).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### How many features (words) are never observed in ham emails? (zero-freq problem)\n",
    "(prob_ham == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementing laplace smoothing\n",
    "G = len(np.unique(y_train))\n",
    "alpha = 1\n",
    "prob_ham = (occurrences_ham + alpha) / (is_ham.sum() + alpha*G)\n",
    "prob_spam = (occurrences_spam  + alpha) / ((~is_ham).sum() + alpha*G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No probabilities equal to zero\n",
    "(prob_ham == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.668, 0.332])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior = np.array([is_ham.sum(), (~is_ham).sum()]) / len(y_train)\n",
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3075466849657944e-159"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_ham = np.array([prob_ham[i] if arr[0,i] == 1 else 1 - prob_ham[i] for i in range(len(prob_ham))])\n",
    "likelihood_ham.prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0531150540210425e-153"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_spam = np.array([prob_spam[i] if arr[0,i] == 1 else 1 - prob_spam[i] for i in range(len(prob_spam))])\n",
    "likelihood_spam.prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.30754668e-159, 4.05311505e-153])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood = np.vstack((likelihood_ham, likelihood_spam),)\n",
    "likelihood.prod(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-365.84287716, -350.89603349])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglikelihood = np.log(likelihood)\n",
    "loglikelihood.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.73441186e-160, 1.34563420e-153])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator = likelihood.prod(axis=1) * prior # unsafe\n",
    "numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-366.24634427, -351.9986538 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lognumerator = loglikelihood.sum(axis=1) + np.log(prior)\n",
    "lognumerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.49092168e-07, 9.99999351e-01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior = numerator / numerator.sum()\n",
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3456350713761717e-153"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.42476911e+01, -6.49092385e-07])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logposterior = lognumerator - np.log(numerator.sum()) # unsafe\n",
    "logposterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.49092168e-07, 9.99999351e-01])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior = np.exp(logposterior)\n",
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999932"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.42476911e+01, -6.49092385e-07])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logposterior = lognumerator - logsumexp(lognumerator) # more stable\n",
    "logposterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.49092168e-07, 9.99999351e-01])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior = np.exp(logposterior)\n",
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999932"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
