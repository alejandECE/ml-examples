{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils.reports as rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads dataset\n",
    "dataset = sio.loadmat('../../datasets/classification/ionosphere.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ionosphere dataset from the UCI machine learning repository:                   ',\n",
       "       'http://archive.ics.uci.edu/ml/datasets/Ionosphere                              ',\n",
       "       'X is a 351x34 real-valued matrix of predictors. Y is a categorical response:   ',\n",
       "       '\"b\" for bad radar returns and \"g\" for good radar returns.                      ',\n",
       "       'This is a binary classification problem.                                       '],\n",
       "      dtype='<U79')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints dataset description\n",
    "dataset['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares data for classification\n",
    "predictors = dataset['X'][:,2:] # the first two columns are useless\n",
    "positive = (dataset['Y'] == 'g').flatten()\n",
    "responses = np.zeros(predictors.shape[0])\n",
    "responses[positive] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits dataset into train/test\n",
    "X,X_holdout,y,y_holdout = train_test_split(predictors,responses,test_size = 0.3,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('normalizer',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=300,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='elasticnet', random_state=None,\n",
       "                                    solver='saga', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates pipeline\n",
    "clf = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=300)\n",
    "sc = StandardScaler()\n",
    "estimators = [('normalizer', sc), ('classifier', clf)]\n",
    "pipe = Pipeline(estimators)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = [{\n",
    "    'classifier__C': np.linspace(0.001,1,25),\n",
    "    'classifier__l1_ratio': np.linspace(0,1,10)\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 250 candidates, totalling 1250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 920 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1227 out of 1250 | elapsed:    4.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1250 out of 1250 | elapsed:    4.9s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    }
   ],
   "source": [
    "validator = GridSearchCV(pipe,cv=5, param_grid=hyperparams, scoring='accuracy', n_jobs=-1,verbose = 1,iid = False)\n",
    "results = validator.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.878 (std: 0.029)\n",
      "Parameters: {'classifier__C': 0.209125, 'classifier__l1_ratio': 0.0}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.873 (std: 0.033)\n",
      "Parameters: {'classifier__C': 0.292375, 'classifier__l1_ratio': 0.0}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.873 (std: 0.030)\n",
      "Parameters: {'classifier__C': 0.1675, 'classifier__l1_ratio': 0.0}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.873 (std: 0.030)\n",
      "Parameters: {'classifier__C': 0.41725, 'classifier__l1_ratio': 0.0}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.873 (std: 0.030)\n",
      "Parameters: {'classifier__C': 0.45887500000000003, 'classifier__l1_ratio': 0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rp.report_grid_search(validator.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test (Metrics): \n",
      "\n",
      "Accuracy:  0.86\n",
      "F1 Score:  0.89\n",
      "Recall:  0.81\n",
      "Precision:  0.98\n",
      "\n",
      "Confusion Matrix:\n",
      " [[30  1]\n",
      " [14 61]]\n"
     ]
    }
   ],
   "source": [
    "# Perform evaluation in the holdout set\n",
    "y_pred = validator.predict(X_holdout)\n",
    "rp.report_classification(y_pred, y_holdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
